{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"d7tb-WciaID5","colab_type":"code","colab":{}},"source":["\"\"\"\n","    Trains the pix2pix model given a Dataset (Dataset.npz)\n","\"\"\"\n","#-*- coding: utf-8 -*- \n","%tensorflow_version 1.x\n","import numpy as np\n","from numpy import load\n","from numpy import zeros\n","from numpy import ones\n","from numpy.random import randint\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.initializers import RandomNormal\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import Conv2DTranspose\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Concatenate\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import LeakyReLU\n","from matplotlib import pyplot\n","from matplotlib.colors import NoNorm\n","\n","\n","from google.colab import drive\n","from google.colab import files\n","import sys"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s_HXJeTmf05f","colab_type":"code","colab":{}},"source":["# define the discriminator model\n","def define_discriminator(in_image_shape, out_image_shape):\n","    # weight initialization\n","    init = RandomNormal(stddev=0.02)\n","    # source image input\n","    in_src_image = Input(shape=in_image_shape)\n","    # target image input\n","    in_target_image = Input(shape=out_image_shape)\n","    # concatenate images channel-wise\n","    merged = Concatenate()([in_src_image, in_target_image])\n","    # C64\n","    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # C128\n","    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","    d = BatchNormalization()(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # C256\n","    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","    d = BatchNormalization()(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # C512\n","    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n","    d = BatchNormalization()(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # second last output layer\n","    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n","    d = BatchNormalization()(d)\n","    d = LeakyReLU(alpha=0.2)(d)\n","    # patch output\n","    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n","    patch_out = Activation('sigmoid')(d)\n","    # define model\n","    model = Model([in_src_image, in_target_image], patch_out)\n","    # compile model\n","    opt = Adam(lr=0.0002, beta_1=0.5)\n","    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qNgYj1Rf7X8","colab_type":"code","colab":{}},"source":["# define an encoder block\n","def define_encoder_block(layer_in, n_filters, batchnorm=True):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# add downsampling layer\n","\tg = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n","\t# conditionally add batch normalization\n","\tif batchnorm:\n","\t\tg = BatchNormalization()(g, training=True)\n","\t# leaky relu activation\n","\tg = LeakyReLU(alpha=0.2)(g)\n","\treturn g"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNB2_2Kqf9xO","colab_type":"code","colab":{}},"source":["# define a decoder block\n","def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n","\t# weight initialization\n","\tinit = RandomNormal(stddev=0.02)\n","\t# add upsampling layer\n","\tg = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n","\t# add batch normalization\n","\tg = BatchNormalization()(g, training=True)\n","\t# conditionally add dropout\n","\tif dropout:\n","\t\tg = Dropout(0.5)(g, training=True)\n","\t# merge with skip connection\n","\tg = Concatenate()([g, skip_in])\n","\t# relu activation\n","\tg = Activation('relu')(g)\n","\treturn g"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"inJQZ8NRgAXg","colab_type":"code","colab":{}},"source":["# define the standalone generator model\n","def define_generator(in_image_shape=(256,256,3)):\n","    # weight initialization\n","    init = RandomNormal(stddev=0.02)\n","    # image input\n","    in_image = Input(shape=in_image_shape)\n","    # encoder model\n","    e1 = define_encoder_block(in_image, 64, batchnorm=False)\n","    e2 = define_encoder_block(e1, 128)\n","    e3 = define_encoder_block(e2, 256)\n","    e4 = define_encoder_block(e3, 512)\n","    e5 = define_encoder_block(e4, 512)\n","    e6 = define_encoder_block(e5, 512)\n","    e7 = define_encoder_block(e6, 512)\n","    # bottleneck, no batch norm and relu\n","    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n","    b = Activation('relu')(b)\n","    # decoder model\n","    d1 = decoder_block(b, e7, 512)\n","    d2 = decoder_block(d1, e6, 512)\n","    d3 = decoder_block(d2, e5, 512)\n","    d4 = decoder_block(d3, e4, 512, dropout=False)\n","    d5 = decoder_block(d4, e3, 256, dropout=False)\n","    d6 = decoder_block(d5, e2, 128, dropout=False)\n","    d7 = decoder_block(d6, e1, 64, dropout=False)\n","    # output\n","    g = Conv2DTranspose(1, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n","    out_image = Activation('tanh')(g)\n","    # define model\n","    model = Model(in_image, out_image)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YehpR23TgCtT","colab_type":"code","colab":{}},"source":["# define the combined generator and discriminator model, for updating the generator\n","def define_gan(g_model, d_model, in_image_shape):\n","    # make weights in the discriminator not trainable\n","    d_model.trainable = False\n","    # define the source image\n","    in_src = Input(shape=in_image_shape)\n","    # connect the source image to the generator input\n","    gen_out = g_model(in_src)\n","    # connect the source input and generator output to the discriminator input\n","    dis_out = d_model([in_src, gen_out])\n","    # src image as input, generated image and classification output\n","    model = Model(in_src, [dis_out, gen_out])\n","    # compile model\n","    opt = Adam(lr=0.0002, beta_1=0.5)\n","    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"55Bh-2wAgFUA","colab_type":"code","colab":{}},"source":["# load and prepare training images\n","def load_real_samples(filename):\n","\t# load compressed arrays\n","\tdata = load(filename)\n","\t# unpack arrays\n","\tX1, X2 = data['arr_0'], data['arr_1']\n","\t# scale from [0,255] to [-1,1]\n","\tX1 = (X1 - 127.5) / 127.5\n","\tX2 = (X2 - 127.5) / 127.5\n","\treturn [X1, X2]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bcQ2Prm5gHqy","colab_type":"code","colab":{}},"source":["# select a batch of random samples, returns images and target\n","def generate_real_samples(dataset, n_samples, patch_shape):\n","\t# unpack dataset\n","\ttrainA, trainB = dataset\n","\t# choose random instances\n","\tix = randint(0, trainA.shape[0], n_samples)\n","\t# retrieve selected images\n","\tX1, X2 = trainA[ix], trainB[ix]\n","\t# generate 'real' class labels (1)\n","\ty = ones((n_samples, patch_shape, patch_shape, 1))\n","\treturn [X1, X2], y\n","\n","# generate a batch of images, returns images and targets\n","def generate_fake_samples(g_model, samples, patch_shape):\n","\t# generate fake instance\n","\tX = g_model.predict(samples)\n","\t# create 'fake' class labels (0)\n","\ty = zeros((len(X), patch_shape, patch_shape, 1))\n","\treturn X, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xv87-2VPgLPF","colab_type":"code","colab":{}},"source":["# generate samples and save as a plot and save the model\n","def summarize_performance(step, g_model, dataset, n_samples=3):\n","    path = '/content/drive/My Drive/ImageToDEM/'\n","    # select a sample of input images\n","    [X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n","    # generate a batch of fake samples\n","    X_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n","    # scale all pixels from [-1,1] to [0,1]\n","    X_realA = (X_realA + 1) / 2.0\n","    X_realB = (X_realB + 1) / 2.0\n","    X_fakeB = (X_fakeB + 1) / 2.0\n","    # plot real source images\n","    for i in range(n_samples):\n","        pyplot.subplot(3, n_samples, 1 + i)\n","        pyplot.axis('off')\n","        pyplot.imshow(X_realA[i])\n","    # plot generated target image\n","    for i in range(n_samples):\n","        pyplot.subplot(3, n_samples, 1 + n_samples + i)\n","        pyplot.axis('off')\n","        # we need to get rid of the last dimension:\n","        im = X_fakeB[i][:,:,0]\n","        pyplot.imshow(im, cmap='gray', norm=NoNorm())\n","    # plot real target image\n","    for i in range(n_samples):\n","        pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n","        pyplot.axis('off')\n","        # we need to get rid of the last dimension:\n","        im = X_realB[i][:,:,0]\n","        pyplot.imshow(im, cmap='gray', norm=NoNorm())\n","    # save plot to file\n","    filename1 = 'plot_%06d.jpg' % (step+1)\n","    pyplot.savefig(path + filename1)\n","    pyplot.close()\n","    # save the generator model\n","    filename2 = 'model_%06d.h5' % (step+1)\n","    g_model.save(path + filename2)\n","    print('>Saved: %s and %s' % (filename1, filename2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVnEkd7sgN-I","colab_type":"code","colab":{}},"source":["# train pix2pix models\n","def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1):\n","\t# determine the output square shape of the discriminator\n","\tn_patch = d_model.output_shape[1]\n","\t# unpack dataset\n","\ttrainA, trainB = dataset\n","\t# calculate the number of batches per training epoch\n","\tbat_per_epo = int(len(trainA) / n_batch)\n","\tprint(\"Bat per epo: \" + str(bat_per_epo))\n","\t# calculate the number of training iterations\n","\tn_steps = bat_per_epo * n_epochs\n","\t# manually enumerate epochs\n","\tsaved = 0\n","\tused = 0\n","\ttimes_bat = 5\n","\tperc_print = 5\n","\tperc = []\n","\tprint(\"Percentage done until next save: \", end='')\n","\tfor i in range(n_steps):\n","\t\t# select a batch of real samples\n","\t\t[X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n","\t\t# generate a batch of fake samples\n","\t\tX_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n","\t\t# update discriminator for real samples\n","\t\td_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n","\t\t# update discriminator for generated samples\n","\t\td_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n","\t\t# update the generator\n","\t\tg_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n","\t\t# summarize performance\n","\t\t# summarize model performance\n","\t\tcurr_perc = int(100 * (saved * (bat_per_epo * times_bat) + ( (i+1) % (bat_per_epo * times_bat * np.amax([1, saved]))) - used )/((saved + 1) * (bat_per_epo * times_bat) - used))\n","\t\tif( curr_perc % perc_print == 0 and not curr_perc in perc):\n","\t\t\tprint(str(curr_perc) + \"% \", end='')\n","\t\t\tperc.append(curr_perc)\n","\t\tif (i+1) % (bat_per_epo * times_bat) == 0 or i == 0:\n","\t\t\tprint()\n","\t\t\tif(i > 0 ): \n","\t\t\t\tsaved = saved + 1\n","\t\t\t\tused = (saved * (bat_per_epo * times_bat) + ( (i+1) % (bat_per_epo * times_bat * np.amax([1, saved]))))\n","\t\t\tprint('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n","\t\t\tprint(\"Total Training done: \" + str(i) + \"/\" + str(n_steps) + \" = \" + str(100.0 * i/n_steps) + \" %\")\n","\t\t\tsummarize_performance(i, g_model, dataset)\n","\t\t\tperc = []\n","\t\t\tprint(\"Percentage done until next save: \", end='')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yO1WCspegQJK","colab_type":"code","outputId":"6d33fb2c-0f7f-44d8-927a-4ddd042b02e4","executionInfo":{"status":"error","timestamp":1572793866073,"user_tz":-120,"elapsed":2064623,"user":{"displayName":"manolis panagiotou","photoUrl":"","userId":"09197886876966874303"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["drive.mount('/content/drive')\n","path = '/content/drive/My Drive/ImageToDEM/'\n","sys.path.append(path)\n","# load image data\n","dataset = load_real_samples(path + 'Dataset_1200.npz')\n","print('Loaded', dataset[0].shape, dataset[1].shape)\n","# define input shape based on the loaded dataset\n","in_image_shape = dataset[0].shape[1:]\n","out_image_shape = dataset[1].shape[1:]\n","# out_image_shape = in_image_shape\n","# dataset[1] = dataset[0]\n","# define the models\n","d_model = define_discriminator(in_image_shape, out_image_shape)\n","g_model = define_generator(in_image_shape)\n","# define the composite model\n","gan_model = define_gan(g_model, d_model, in_image_shape)\n","# train model\n","train(d_model, g_model, gan_model, dataset)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Loaded (1215, 256, 256, 3) (1215, 256, 256, 1)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Bat per epo: 1215\n","Percentage done until next save: WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","0% \n",">1, d1[0.338] d2[0.905] g[38.631]\n","Total Training done: 0/121500 = 0.0 %\n",">Saved: plot_000001.jpg and model_000001.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% \n",">6075, d1[0.010] d2[0.002] g[26.478]\n","Total Training done: 6074/121500 = 4.99917695473251 %\n",">Saved: plot_006075.jpg and model_006075.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% \n",">12150, d1[0.010] d2[0.003] g[20.945]\n","Total Training done: 12149/121500 = 9.99917695473251 %\n",">Saved: plot_012150.jpg and model_012150.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">18225, d1[0.393] d2[0.002] g[24.641]\n","Total Training done: 18224/121500 = 14.99917695473251 %\n",">Saved: plot_018225.jpg and model_018225.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">24300, d1[0.020] d2[0.019] g[11.027]\n","Total Training done: 24299/121500 = 19.999176954732512 %\n",">Saved: plot_024300.jpg and model_024300.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">30375, d1[0.011] d2[0.394] g[5.830]\n","Total Training done: 30374/121500 = 24.999176954732512 %\n",">Saved: plot_030375.jpg and model_030375.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">36450, d1[0.000] d2[0.005] g[13.953]\n","Total Training done: 36449/121500 = 29.999176954732512 %\n",">Saved: plot_036450.jpg and model_036450.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">42525, d1[0.068] d2[0.010] g[10.654]\n","Total Training done: 42524/121500 = 34.99917695473251 %\n",">Saved: plot_042525.jpg and model_042525.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">48600, d1[0.016] d2[0.053] g[10.506]\n","Total Training done: 48599/121500 = 39.99917695473251 %\n",">Saved: plot_048600.jpg and model_048600.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">54675, d1[0.269] d2[0.079] g[5.048]\n","Total Training done: 54674/121500 = 44.99917695473251 %\n",">Saved: plot_054675.jpg and model_054675.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">60750, d1[0.051] d2[0.551] g[14.486]\n","Total Training done: 60749/121500 = 49.99917695473251 %\n",">Saved: plot_060750.jpg and model_060750.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">66825, d1[0.096] d2[0.083] g[10.242]\n","Total Training done: 66824/121500 = 54.99917695473251 %\n",">Saved: plot_066825.jpg and model_066825.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">72900, d1[0.152] d2[0.000] g[18.151]\n","Total Training done: 72899/121500 = 59.99917695473251 %\n",">Saved: plot_072900.jpg and model_072900.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">78975, d1[0.000] d2[0.155] g[5.721]\n","Total Training done: 78974/121500 = 64.9991769547325 %\n",">Saved: plot_078975.jpg and model_078975.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">85050, d1[0.075] d2[0.021] g[8.623]\n","Total Training done: 85049/121500 = 69.9991769547325 %\n",">Saved: plot_085050.jpg and model_085050.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">91125, d1[0.129] d2[0.088] g[10.230]\n","Total Training done: 91124/121500 = 74.9991769547325 %\n",">Saved: plot_091125.jpg and model_091125.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">97200, d1[0.120] d2[0.091] g[5.359]\n","Total Training done: 97199/121500 = 79.9991769547325 %\n",">Saved: plot_097200.jpg and model_097200.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% \n",">103275, d1[0.020] d2[0.000] g[14.008]\n","Total Training done: 103274/121500 = 84.9991769547325 %\n",">Saved: plot_103275.jpg and model_103275.h5\n","Percentage done until next save: 0% 5% 10% 15% 20% 25% 30% "],"name":"stdout"}]}]}